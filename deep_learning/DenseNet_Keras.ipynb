{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_data = \"/media/diego/QData/datasets/traffic_signs/\"\n",
    "train_data = root_data + \"train.p\"\n",
    "valid_data = root_data + \"valid.p\"\n",
    "test_data = root_data + \"test.p\"\n",
    "results = root_data + \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "from keras.backend import tf as k\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from utils import *\n",
    "import importlib\n",
    "import utils2; importlib.reload(utils2)\n",
    "from utils2 import *\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "with open(train_data, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(test_data, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "with open(valid_data, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (34799, 32, 32, 3)\n",
      "34799 train samples\n",
      "4410 valid samples\n",
      "12630 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_valid /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'valid samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt4AAAEICAYAAABoG/PwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9251374b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "train_label_counter = Counter(y_train)\n",
    "\n",
    "train_counter = Counter(y_train)\n",
    "order = list(zip(*train_counter.most_common()))[0]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 4))\n",
    "ax = sns.countplot(x=y_train, order=order, color='lightblue', ax=ax, label=\"train\")\n",
    "\n",
    "_ = ax.set_title('Class distribution')\n",
    "_ = ax.legend(ncol=2, loc=\"upper right\", frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34799, 43)\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, 43)\n",
    "Y_valid = np_utils.to_categorical(y_valid, 43)\n",
    "Y_test = np_utils.to_categorical(y_test, 43)\n",
    "\n",
    "print (Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train_samples = X_train.shape[0]\n",
    "nb_valid_samples = X_valid.shape[0]\n",
    "batch_size = 32\n",
    "steps_per_epoch = nb_train_samples // batch_size\n",
    "valid_steps = nb_valid_samples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x): return Activation('relu')(x)\n",
    "def dropout(x, p): return Dropout(p)(x) if p else x\n",
    "def bn(x): return BatchNormalization()(x)\n",
    "def relu_bn(x): return relu(bn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(x, nf, sz, wd, p):\n",
    "    x = Conv2D(nf, (sz, sz), kernel_initializer=\"he_uniform\", padding='same', \n",
    "                          kernel_regularizer=regularizers.l2(wd))(x)\n",
    "    return dropout(x,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(x, nf, bottleneck=False, p=None, wd=0):\n",
    "    x = relu_bn(x)\n",
    "    if bottleneck: x = relu_bn(conv(x, nf * 4, 1, wd, p))\n",
    "    return conv(x, nf, 3, wd, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_block(x, nb_layers, growth_rate, bottleneck=False, p=None, wd=0):\n",
    "    if bottleneck: nb_layers //= 2\n",
    "    for i in range(nb_layers):\n",
    "        b = conv_block(x, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
    "        x = merge([x,b], mode='concat', concat_axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transition_block(x, compression=1.0, p=None, wd=0):\n",
    "    nf = int(x.get_shape().as_list()[-1] * compression)\n",
    "    x = relu_bn(x)\n",
    "    x = conv(x, nf, 1, wd, p)\n",
    "    return AveragePooling2D((2, 2), strides=(2, 2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dense_net(nb_classes, img_input, depth=40, nb_block=3, \n",
    "     growth_rate=12, nb_filter=16, bottleneck=False, compression=1.0, p=None, wd=0, activation='softmax'):\n",
    "    \n",
    "    assert activation == 'softmax' or activation == 'sigmoid'\n",
    "    assert (depth - 4) % nb_block == 0\n",
    "    nb_layers_per_block = int((depth - 4) / nb_block)\n",
    "    nb_layers = [nb_layers_per_block] * nb_block\n",
    "\n",
    "    x = conv(img_input, nb_filter, 3, wd, 0)\n",
    "    for i,block in enumerate(nb_layers):\n",
    "        x = dense_block(x, block, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
    "        if i != len(nb_layers)-1:\n",
    "            x = transition_block(x, compression=compression, p=p, wd=wd)\n",
    "\n",
    "    x = relu_bn(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Dense(nb_classes, activation=activation, kernel_regularizer=regularizers.l2(wd))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/dsretreat/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \"\"\"\n",
      "/home/diego/anaconda3/envs/dsretreat/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "x = create_dense_net(43, img_input, depth=100, nb_filter=16, compression=0.5, \n",
    "                     bottleneck=True, p=0.2, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(img_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "      optimizer=keras.optimizers.SGD(0.1, 0.9, nesterov=True), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/20\n",
      "34799/34799 [==============================] - 210s - loss: 2.6717 - acc: 0.3900 - val_loss: 3.0927 - val_acc: 0.4354\n",
      "Epoch 2/20\n",
      "34799/34799 [==============================] - 195s - loss: 0.9215 - acc: 0.8911 - val_loss: 1.4083 - val_acc: 0.8039\n",
      "Epoch 3/20\n",
      "34799/34799 [==============================] - 192s - loss: 0.6249 - acc: 0.9691 - val_loss: 1.0489 - val_acc: 0.8637\n",
      "Epoch 4/20\n",
      "34799/34799 [==============================] - 193s - loss: 0.5086 - acc: 0.9829 - val_loss: 0.8672 - val_acc: 0.8943\n",
      "Epoch 5/20\n",
      "34799/34799 [==============================] - 196s - loss: 0.4359 - acc: 0.9861 - val_loss: 0.8004 - val_acc: 0.8959\n",
      "Epoch 6/20\n",
      "34799/34799 [==============================] - 191s - loss: 0.3756 - acc: 0.9897 - val_loss: 0.5253 - val_acc: 0.9506\n",
      "Epoch 7/20\n",
      "34799/34799 [==============================] - 191s - loss: 0.3302 - acc: 0.9903 - val_loss: 0.4271 - val_acc: 0.9630\n",
      "Epoch 8/20\n",
      "34799/34799 [==============================] - 194s - loss: 0.2904 - acc: 0.9915 - val_loss: 1.0360 - val_acc: 0.8254\n",
      "Epoch 9/20\n",
      "34799/34799 [==============================] - 193s - loss: 0.2615 - acc: 0.9918 - val_loss: 0.5383 - val_acc: 0.9408\n",
      "Epoch 10/20\n",
      "34799/34799 [==============================] - 193s - loss: 0.2372 - acc: 0.9915 - val_loss: 0.4692 - val_acc: 0.9356\n",
      "Epoch 11/20\n",
      "34799/34799 [==============================] - 195s - loss: 0.2209 - acc: 0.9922 - val_loss: 0.4958 - val_acc: 0.9365\n",
      "Epoch 12/20\n",
      "34799/34799 [==============================] - 196s - loss: 0.2030 - acc: 0.9933 - val_loss: 0.2950 - val_acc: 0.9694\n",
      "Epoch 13/20\n",
      "34799/34799 [==============================] - 197s - loss: 0.1838 - acc: 0.9945 - val_loss: 0.3246 - val_acc: 0.9603\n",
      "Epoch 14/20\n",
      "34799/34799 [==============================] - 202s - loss: 0.1752 - acc: 0.9932 - val_loss: 0.4223 - val_acc: 0.9247\n",
      "Epoch 15/20\n",
      "34799/34799 [==============================] - 197s - loss: 0.1685 - acc: 0.9927 - val_loss: 0.7194 - val_acc: 0.8823\n",
      "Epoch 16/20\n",
      "34799/34799 [==============================] - 197s - loss: 0.1650 - acc: 0.9932 - val_loss: 0.3159 - val_acc: 0.9488\n",
      "Epoch 17/20\n",
      "23168/34799 [==================>...........] - ETA: 62s - loss: 0.1467 - acc: 0.9962"
     ]
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, 0.1)\n",
    "model.fit(X_train, y_train, 64, 20, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(results+'first_model.hp5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/4\n",
      "34799/34799 [==============================] - 289s - loss: 0.1276 - acc: 0.9989 - val_loss: 0.1618 - val_acc: 0.9884\n",
      "Epoch 2/4\n",
      "34799/34799 [==============================] - 285s - loss: 0.1230 - acc: 0.9995 - val_loss: 0.1585 - val_acc: 0.9878\n",
      "Epoch 3/4\n",
      "34799/34799 [==============================] - 288s - loss: 0.1196 - acc: 0.9997 - val_loss: 0.1505 - val_acc: 0.9891\n",
      "Epoch 4/4\n",
      "34799/34799 [==============================] - 288s - loss: 0.1164 - acc: 0.9999 - val_loss: 0.1472 - val_acc: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f512f91f9b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, 0.01)\n",
    "model.fit(X_train, y_train, 64, 4, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.1389 - acc: 0.9936 - val_loss: 0.4966 - val_acc: 0.9254\n",
      "Epoch 2/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1400 - acc: 0.9935 - val_loss: 0.4093 - val_acc: 0.9392\n",
      "Epoch 3/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1295 - acc: 0.9961 - val_loss: 0.3114 - val_acc: 0.9492\n",
      "Epoch 4/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1285 - acc: 0.9951 - val_loss: 0.2037 - val_acc: 0.9748\n",
      "Epoch 5/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1321 - acc: 0.9941 - val_loss: 0.5978 - val_acc: 0.9132\n",
      "Epoch 6/20\n",
      "34799/34799 [==============================] - 290s - loss: 0.1392 - acc: 0.9932 - val_loss: 0.2569 - val_acc: 0.9571\n",
      "Epoch 7/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1291 - acc: 0.9963 - val_loss: 0.5426 - val_acc: 0.9091\n",
      "Epoch 8/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.1278 - acc: 0.9956 - val_loss: 0.1867 - val_acc: 0.9810\n",
      "Epoch 9/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.1141 - acc: 0.9969 - val_loss: 0.2192 - val_acc: 0.9664\n",
      "Epoch 10/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1183 - acc: 0.9959 - val_loss: 0.2185 - val_acc: 0.9680\n",
      "Epoch 11/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1325 - acc: 0.9926 - val_loss: 0.2938 - val_acc: 0.9526\n",
      "Epoch 12/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1239 - acc: 0.9955 - val_loss: 0.2358 - val_acc: 0.9694\n",
      "Epoch 13/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1233 - acc: 0.9945 - val_loss: 0.4429 - val_acc: 0.9204\n",
      "Epoch 14/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1174 - acc: 0.9966 - val_loss: 0.1746 - val_acc: 0.9762\n",
      "Epoch 15/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1192 - acc: 0.9949 - val_loss: 0.2650 - val_acc: 0.9553\n",
      "Epoch 16/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1214 - acc: 0.9952 - val_loss: 0.1675 - val_acc: 0.9841\n",
      "Epoch 17/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1028 - acc: 0.9985 - val_loss: 0.2440 - val_acc: 0.9501\n",
      "Epoch 18/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1169 - acc: 0.9948 - val_loss: 0.1447 - val_acc: 0.9864\n",
      "Epoch 19/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1183 - acc: 0.9947 - val_loss: 0.6680 - val_acc: 0.8889\n",
      "Epoch 20/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1182 - acc: 0.9953 - val_loss: 0.2923 - val_acc: 0.9515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5117a85c50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, 0.1)\n",
    "model.fit(X_train, y_train, 64, 20, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.1055 - acc: 0.9986 - val_loss: 0.1257 - val_acc: 0.9934\n",
      "Epoch 2/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0995 - acc: 0.9996 - val_loss: 0.1282 - val_acc: 0.9923\n",
      "Epoch 3/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0971 - acc: 0.9998 - val_loss: 0.1201 - val_acc: 0.9948\n",
      "Epoch 4/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0946 - acc: 0.9998 - val_loss: 0.1161 - val_acc: 0.9966\n",
      "Epoch 5/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0922 - acc: 1.0000 - val_loss: 0.1137 - val_acc: 0.9959\n",
      "Epoch 6/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0904 - acc: 1.0000 - val_loss: 0.1136 - val_acc: 0.9941\n",
      "Epoch 7/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0886 - acc: 0.9999 - val_loss: 0.1141 - val_acc: 0.9937\n",
      "Epoch 8/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0865 - acc: 1.0000 - val_loss: 0.1073 - val_acc: 0.9964\n",
      "Epoch 9/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0848 - acc: 0.9999 - val_loss: 0.1084 - val_acc: 0.9955\n",
      "Epoch 10/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0830 - acc: 0.9999 - val_loss: 0.1058 - val_acc: 0.9946\n",
      "Epoch 11/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0812 - acc: 0.9999 - val_loss: 0.1017 - val_acc: 0.9964\n",
      "Epoch 12/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0796 - acc: 0.9999 - val_loss: 0.1007 - val_acc: 0.9955\n",
      "Epoch 13/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0777 - acc: 0.9999 - val_loss: 0.0997 - val_acc: 0.9964\n",
      "Epoch 14/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0761 - acc: 0.9999 - val_loss: 0.0976 - val_acc: 0.9961\n",
      "Epoch 15/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0745 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9959\n",
      "Epoch 16/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0730 - acc: 0.9999 - val_loss: 0.0948 - val_acc: 0.9964\n",
      "Epoch 17/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0715 - acc: 1.0000 - val_loss: 0.0925 - val_acc: 0.9964\n",
      "Epoch 18/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0701 - acc: 0.9999 - val_loss: 0.0925 - val_acc: 0.9957\n",
      "Epoch 19/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0688 - acc: 0.9999 - val_loss: 0.0883 - val_acc: 0.9959\n",
      "Epoch 20/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0671 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5117a85c88>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, 0.01)\n",
    "model.fit(X_train, y_train, 64, 20, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/20\n",
      "34799/34799 [==============================] - 287s - loss: 0.0664 - acc: 1.0000 - val_loss: 0.0876 - val_acc: 0.9961\n",
      "Epoch 2/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 0.9961\n",
      "Epoch 3/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0660 - acc: 0.9999 - val_loss: 0.0866 - val_acc: 0.9961\n",
      "Epoch 4/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0659 - acc: 1.0000 - val_loss: 0.0864 - val_acc: 0.9961\n",
      "Epoch 5/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0657 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9964\n",
      "Epoch 6/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0656 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9964\n",
      "Epoch 7/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0655 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9964\n",
      "Epoch 8/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9961\n",
      "Epoch 9/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0652 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9964\n",
      "Epoch 10/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0650 - acc: 0.9999 - val_loss: 0.0856 - val_acc: 0.9964\n",
      "Epoch 11/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0648 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9964\n",
      "Epoch 12/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0647 - acc: 0.9999 - val_loss: 0.0848 - val_acc: 0.9964\n",
      "Epoch 13/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0646 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9964\n",
      "Epoch 14/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0645 - acc: 1.0000 - val_loss: 0.0845 - val_acc: 0.9964\n",
      "Epoch 15/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0643 - acc: 1.0000 - val_loss: 0.0841 - val_acc: 0.9964\n",
      "Epoch 16/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0642 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 0.9964\n",
      "Epoch 17/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0641 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 0.9964\n",
      "Epoch 18/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0641 - acc: 0.9999 - val_loss: 0.0833 - val_acc: 0.9964\n",
      "Epoch 19/20\n",
      "34799/34799 [==============================] - 288s - loss: 0.0638 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9964\n",
      "Epoch 20/20\n",
      "34799/34799 [==============================] - 289s - loss: 0.0637 - acc: 0.9999 - val_loss: 0.0832 - val_acc: 0.9964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5117a85d68>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, 0.001)\n",
    "model.fit(X_train, y_train, 64, 20, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
